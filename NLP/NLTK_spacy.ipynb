{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WK5IADlqKN_u",
        "outputId": "b2f31872-34f5-49aa-a596-76d78e2c3aec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.11.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.14)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: pathlib-abc==0.1.1 in /usr/local/lib/python3.10/dist-packages (from pathy>=0.10.0->spacy) (0.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import spacy\n"
      ],
      "metadata": {
        "id": "dEZYFPFeKhpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('words')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('averaged_perceptron_tagger')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwZ8Q6o4MiFw",
        "outputId": "b811a52a-0275-440e-d9a4-adb0a5918fe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Tokenizing**"
      ],
      "metadata": {
        "id": "FYgLzRFro_nJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize, wordpunct_tokenize,TreebankWordTokenizer\n",
        "text= \"stopwords are common words that are often removed from text during the preprocessing phase of natural language processing task's because they are considered to carry little meaning. It is good.\"\n",
        "tokens = word_tokenize(text)\n",
        "sent = sent_tokenize(text)\n",
        "wordpuc = wordpunct_tokenize(text)\n",
        "tree = TreebankWordTokenizer()\n",
        "tok = tree.tokenize(text)\n",
        "print(tokens)\n",
        "print(sent)\n",
        "print(wordpuc)\n",
        "print(tok)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrcOO1mjLFbj",
        "outputId": "7665e734-fe89-4d6c-d434-c7ef2a308acb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['stopwords', 'are', 'common', 'words', 'that', 'are', 'often', 'removed', 'from', 'text', 'during', 'the', 'preprocessing', 'phase', 'of', 'natural', 'language', 'processing', 'task', \"'s\", 'because', 'they', 'are', 'considered', 'to', 'carry', 'little', 'meaning', '.', 'It', 'is', 'good', '.']\n",
            "[\"stopwords are common words that are often removed from text during the preprocessing phase of natural language processing task's because they are considered to carry little meaning.\", 'It is good.']\n",
            "['stopwords', 'are', 'common', 'words', 'that', 'are', 'often', 'removed', 'from', 'text', 'during', 'the', 'preprocessing', 'phase', 'of', 'natural', 'language', 'processing', 'task', \"'\", 's', 'because', 'they', 'are', 'considered', 'to', 'carry', 'little', 'meaning', '.', 'It', 'is', 'good', '.']\n",
            "['stopwords', 'are', 'common', 'words', 'that', 'are', 'often', 'removed', 'from', 'text', 'during', 'the', 'preprocessing', 'phase', 'of', 'natural', 'language', 'processing', 'task', \"'s\", 'because', 'they', 'are', 'considered', 'to', 'carry', 'little', 'meaning.', 'It', 'is', 'good', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Removing Stopwords**"
      ],
      "metadata": {
        "id": "LphyenZapWuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from  nltk.corpus import stopwords\n",
        "sw=stopwords.words('english')\n",
        "stopwords_removed = [word for word in tokens if word.lower() not in sw]\n",
        "print(stopwords_removed)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OcJmcAYpHHA",
        "outputId": "f8a5a195-d2f9-4ab5-c345-c03030ea960a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['stopwords', 'common', 'words', 'often', 'removed', 'text', 'preprocessing', 'phase', 'natural', 'language', 'processing', 'task', \"'s\", 'considered', 'carry', 'little', 'meaning', '.', 'good', '.']\n",
            "['stopwords', 'are', 'common', 'words', 'that', 'are', 'often', 'removed', 'from', 'text', 'during', 'the', 'preprocessing', 'phase', 'of', 'natural', 'language', 'processing', 'task', \"'s\", 'because', 'they', 'are', 'considered', 'to', 'carry', 'little', 'meaning', '.', 'It', 'is', 'good', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Stemming**"
      ],
      "metadata": {
        "id": "9nDscnuvjQUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from nltk.stem import SnowballStemmer, PorterStemmer, RegexpStemmer\n",
        "from nltk import tokenize\n",
        "\n",
        "ss=SnowballStemmer('english')\n",
        "text = 'Stemming helps to reduce words to their base form, improving text analysis.'\n",
        "tokens = word_tokenize(text)\n",
        "stemmed = [ss.stem(token) for token in tokens]\n",
        "print(tokens)\n",
        "print(stemmed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twEFwtC2Te-o",
        "outputId": "bbe548c8-4a7a-43f9-f2ae-2c1e2bad1a5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Stemming', 'helps', 'to', 'reduce', 'words', 'to', 'their', 'base', 'form', ',', 'improving', 'text', 'analysis', '.']\n",
            "['stem', 'help', 'to', 'reduc', 'word', 'to', 'their', 'base', 'form', ',', 'improv', 'text', 'analysi', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ps = PorterStemmer()\n",
        "tokens = word_tokenize(text)\n",
        "stemmed = [ps.stem(token) for token in tokens]\n",
        "print(tokens)\n",
        "print(stemmed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwmQdRURfOT9",
        "outputId": "056dacfe-673f-4556-c9f4-8628af08bdc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Stemming', 'helps', 'to', 'reduce', 'words', 'to', 'their', 'base', 'form', ',', 'improving', 'text', 'analysis', '.']\n",
            "['stem', 'help', 'to', 'reduc', 'word', 'to', 'their', 'base', 'form', ',', 'improv', 'text', 'analysi', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reg = RegexpStemmer('ing$|s$|e$|able$', min = 4)\n",
        "tokens = word_tokenize(text)\n",
        "stemmed = [reg.stem(token) for token in tokens]\n",
        "print(tokens)\n",
        "print(stemmed)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctnDWDNxgGA1",
        "outputId": "80e40288-02d2-458c-8e17-e681bc9355ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Stemming', 'helps', 'to', 'reduce', 'words', 'to', 'their', 'base', 'form', ',', 'improving', 'text', 'analysis', '.']\n",
            "['Stemm', 'help', 'to', 'reduc', 'word', 'to', 'their', 'bas', 'form', ',', 'improv', 'text', 'analysi', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Lemmatization**"
      ],
      "metadata": {
        "id": "2KymxGWBjY2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lem = WordNetLemmatizer()\n",
        "tokens = word_tokenize(text)\n",
        "tok =[lem.lemmatize(words,pos='v') for words in tokens]\n",
        "print(tokens)\n",
        "print(tok)\n",
        "# v - verb\n",
        "# n - noun\n",
        "# r - adverb\n",
        "# a - adjective\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Ej1Ef8Ujdz7",
        "outputId": "9589cbd9-57a0-4529-e511-178386699520"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Stemming', 'helps', 'to', 'reduce', 'words', 'to', 'their', 'base', 'form', ',', 'improving', 'text', 'analysis', '.']\n",
            "['Stemming', 'help', 'to', 'reduce', 'word', 'to', 'their', 'base', 'form', ',', 'improve', 'text', 'analysis', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Parts of speech**"
      ],
      "metadata": {
        "id": "2USVYleip-ej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tags =nltk.pos_tag(tokens)\n",
        "tags"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6YEZj_PqPA4",
        "outputId": "0ef3e7e1-dc56-41ac-94b7-ce4f11c22a35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('stopwords', 'NNS'),\n",
              " ('are', 'VBP'),\n",
              " ('common', 'JJ'),\n",
              " ('words', 'NNS'),\n",
              " ('that', 'WDT'),\n",
              " ('are', 'VBP'),\n",
              " ('often', 'RB'),\n",
              " ('removed', 'VBN'),\n",
              " ('from', 'IN'),\n",
              " ('text', 'NN'),\n",
              " ('during', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('preprocessing', 'VBG'),\n",
              " ('phase', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('natural', 'JJ'),\n",
              " ('language', 'NN'),\n",
              " ('processing', 'NN'),\n",
              " ('task', 'NN'),\n",
              " (\"'s\", 'POS'),\n",
              " ('because', 'IN'),\n",
              " ('they', 'PRP'),\n",
              " ('are', 'VBP'),\n",
              " ('considered', 'VBN'),\n",
              " ('to', 'TO'),\n",
              " ('carry', 'VB'),\n",
              " ('little', 'JJ'),\n",
              " ('meaning', 'NN'),\n",
              " ('.', '.'),\n",
              " ('It', 'PRP'),\n",
              " ('is', 'VBZ'),\n",
              " ('good', 'JJ'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Name entity tag**"
      ],
      "metadata": {
        "id": "kyKUOr8Grc1M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#text = \"In Paris, John Smith attended a conference on artificial intelligence last Friday at 9.30 pm.\"\n",
        "#tokens = word_tokenize(text)\n",
        "#tags =nltk.pos_tag(tokens)\n",
        "#nltk.ne_chunk(tags).draw()"
      ],
      "metadata": {
        "id": "DvVu9oevrjgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spc = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"In Paris, John Smith attended a conference on artificial intelligence last Friday at 9.30 pm.\"\n",
        "doc = spc(text)\n",
        "entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "print(entities)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrrIbPi9NR05",
        "outputId": "f0b9a32b-605f-4b1c-c288-892963891dbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Paris', 'GPE'), ('John Smith', 'PERSON'), ('last Friday', 'DATE'), ('9.30 pm', 'TIME')]\n"
          ]
        }
      ]
    }
  ]
}